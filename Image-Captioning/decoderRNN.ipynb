{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a86aa62-920b-4626-8a9b-bb4b87859d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hariom/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "from data_loader import get_loader\n",
    "from data_loader_val import get_loader as val_get_loader\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms \n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from nlp_utils import clean_sentence, bleu_score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "# Define EncoderCNN class\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self,embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet= models.resnet50(pretrained=True )\n",
    "\n",
    "# Disabled learning for perameters\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def Forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87899918-fc78-4473-a8e4-3c627f8cf951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class define the decoder part CNN-RNN model for image caption\n",
    "# Decoder\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_size: final embedding size of the CNN encoder\n",
    "            hidden_size: hidden size of the LSTM\n",
    "            vocab_size: size of the vocabulary\n",
    "            num_layers: number of layers of the LSTM\n",
    "        \"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # Assigning hidden dimensions\n",
    "        self.hidden_dim = hidden_size \n",
    "\n",
    "        # Map each word index to a dense word embedding tensor of embed_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # Creating LSTM Layer\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Apply a linear layer to the output of the LSTM\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        # Initialization of the hidden state\n",
    "        self.hidden = (torch.zeros(1, 1, hidden_size), torch.zeros(1, 1, hidden_size))\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: features tensor. shape is (bs, embed_size)\n",
    "            captions: captions tensor. shape is (bs, cap_length)\n",
    "        Returns:\n",
    "            outputs: scores of the linear layer\n",
    "        \"\"\"\n",
    "\n",
    "        # Remove <end> from captions and embed captions\n",
    "        cap_embedding = self.embed(captions[:, :-1])\n",
    "\n",
    "        # Concatenate the features and the caption embeddings\n",
    "        embeddings = torch.cat((features.unsqueeze(dim=1), cap_embedding), dim=1)\n",
    "\n",
    "        # Pass the embeddings through the LSTM\n",
    "        lstm_out, self.hidden = self.lstm(embeddings)\n",
    "\n",
    "        # Apply the linear layer to the LSTM output\n",
    "        outputs = self.linear(lstm_out)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, inputs, states=None, max_len=20):\n",
    "        \"\"\"\n",
    "        Accepts pre-processed image tensor (inputs) and returns predicted\n",
    "        sentence (list of tensor ids of length max_len)\n",
    "        Args:\n",
    "            inputs: shape is (1, 1, embed_size)\n",
    "            states: initial hidden state of the LSTM\n",
    "            max_len: maximum length of the predicted sentence\n",
    "        Returns:\n",
    "            res: list of predicted word indices\n",
    "        \"\"\"\n",
    "        res = []\n",
    "\n",
    "        # Now we feed the LSTM output\n",
    "        for i in range(max_len):\n",
    "            lstm_out, states = self.lstm(inputs, states)\n",
    "            outputs = self.linear(lstm_out.squeeze(dim=1))\n",
    "            _, predicted_idx = outputs.max(dim=1)\n",
    "            res.append(predicted_idx.item())\n",
    "\n",
    "            if predicted_idx == 1:  # Assuming 1 is the index for <end>\n",
    "                break\n",
    "            inputs = self.embed(predicted_idx)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd5175f-f103-4c9a-8758-6dd85fa13b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=5.20s)\n",
      "creating index...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     21\u001b[0m transform_train \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Resize the smaller edge of the image to 256\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m256\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     ),\n\u001b[1;32m     34\u001b[0m ])\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Build dataloader\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m \u001b[43mget_loader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_from_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_from_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcocoapi_loc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcocoapi_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Get vocabulary size\u001b[39;00m\n\u001b[1;32m     47\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mvocab)\n",
      "File \u001b[0;32m~/python/Image detector/Image-Captioning/data_loader.py:72\u001b[0m, in \u001b[0;36mget_loader\u001b[0;34m(transform, mode, batch_size, vocab_threshold, vocab_file, start_word, end_word, unk_word, vocab_from_file, num_workers, cocoapi_loc)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# COCO caption dataset.\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCoCoDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_word\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_word\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_word\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mannotations_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotations_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_from_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_from_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Randomly sample a caption length, and sample indices with that length.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     indices \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mget_train_indices()\n",
      "File \u001b[0;32m~/python/Image detector/Image-Captioning/coco_dataset.py:45\u001b[0m, in \u001b[0;36mCoCoDataset.__init__\u001b[0;34m(self, transform, mode, batch_size, vocab_threshold, vocab_file, start_word, end_word, unk_word, annotations_file, vocab_from_file, img_folder)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m Vocabulary(\n\u001b[1;32m     36\u001b[0m     vocab_threshold,\n\u001b[1;32m     37\u001b[0m     vocab_file,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     vocab_from_file,\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco \u001b[38;5;241m=\u001b[39m \u001b[43mCOCO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotations_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco\u001b[38;5;241m.\u001b[39manns\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObtaining caption lengths...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pycocotools/coco.py:86\u001b[0m, in \u001b[0;36mCOCO.__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone (t=\u001b[39m\u001b[38;5;132;01m{:0.2f}\u001b[39;00m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39m tic))\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m dataset\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pycocotools/coco.py:96\u001b[0m, in \u001b[0;36mCOCO.createIndex\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ann \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     95\u001b[0m         imgToAnns[ann[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mappend(ann)\n\u001b[0;32m---> 96\u001b[0m         anns[\u001b[43mann\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m] \u001b[38;5;241m=\u001b[39m ann\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "# We need to know the length of our dataset vocabulary\n",
    "from data_loader import get_loader\n",
    "from data_loader_val import get_loader as val_get_loader\n",
    "from torchvision import transforms \n",
    "\n",
    "# Dataset directory path\n",
    "cocoapi_dir = r\"../cocoapi/\"\n",
    "\n",
    "# Configuration parameters\n",
    "batch_size = 128  # Batch size\n",
    "vocab_threshold = 5  # Minimum word count threshold\n",
    "vocab_from_file = True  # If True, load existing vocab file\n",
    "embed_size = 256  # Dimensionality of image and word embeddings\n",
    "hidden_size = 512  # Number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3  # Number of training epochs\n",
    "save_every = 1  # Determines frequency of saving model weights\n",
    "print_every = 20  # Determines window for printing average loss\n",
    "log_file = \"training_log.txt\"  # Name of file with saved training loss and perplexity\n",
    "\n",
    "# Image transformations for training\n",
    "transform_train = transforms.Compose([\n",
    "    # Resize the smaller edge of the image to 256\n",
    "    transforms.Resize(256),\n",
    "    # Get a 224x224 crop from a random location\n",
    "    transforms.RandomCrop(224),\n",
    "    # Horizontally flip the image with probability=0.5\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # Convert the PIL image to a tensor\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        (0.485, 0.456, 0.406),\n",
    "        (0.229, 0.224, 0.225),\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Build dataloader\n",
    "data_loader = get_loader(\n",
    "    transform=transform_train,\n",
    "    mode=\"train\",\n",
    "    batch_size=batch_size,\n",
    "    vocab_threshold=vocab_threshold,\n",
    "    vocab_from_file=vocab_from_file,\n",
    "    cocoapi_loc=cocoapi_dir,\n",
    ")\n",
    "\n",
    "# Get vocabulary size\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "print(\"vocab size is: \",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "989ce153-6b3d-40dc-8877-0f9f2ddee93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embed): Embedding(512, 256)\n",
       "  (lstm): LSTM(256, 11543, batch_first=True)\n",
       "  (linear): Linear(in_features=11543, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Define or import vocab_size\n",
    "vocab_size = 11543  # Example value, replace with the actual size of your vocabulary\n",
    "embed_size = 256    # Example value, ensure it's defined\n",
    "hidden_size = 512   # Example value, ensure it's defined\n",
    "\n",
    "# Initialize the encoder and decoder\n",
    "encoder = EncoderCNN(embed_size)  # Make sure EncoderCNN is defined and imported if necessary\n",
    "decoder = DecoderRNN(embed_size, vocab_size, hidden_size)  # Ensure DecoderRNN is defined and imported\n",
    "\n",
    "# Move models to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c7e6a8c-ba37-4be5-8e30-e1023f13612e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the embedding for a single word: torch.Size([256])\n",
      "Shape of the embedding matrix: torch.Size([11543, 256])\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "embed_size = 256  # Dimensionality of word embeddings\n",
    "vocab_size = 11543  # Number of unique words in vocabulary\n",
    "\n",
    "# Instantiate an embedding layer\n",
    "embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "# Example word index\n",
    "word_index = 10  # Example index of a word\n",
    "\n",
    "# Get the embedding for this word\n",
    "word_embedding = embed(torch.tensor(word_index))\n",
    "\n",
    "# Print the shape of the embedding for a single word\n",
    "print(\"Shape of the embedding for a single word:\", word_embedding.shape)\n",
    "\n",
    "# Get the whole embedding matrix\n",
    "embedding_matrix = embed.weight.data\n",
    "\n",
    "# Print the shape of the embedding matrix\n",
    "print(\"Shape of the embedding matrix:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca34987-06a0-4bee-b881-f5e4db4207a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
