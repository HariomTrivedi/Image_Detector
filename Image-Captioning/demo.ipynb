{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "# Decoder\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden = (torch.zeros(num_layers, 1, hidden_size), torch.zeros(num_layers, 1, hidden_size))\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        cap_embedding = self.embed(captions[:, :-1])\n",
    "        embeddings = torch.cat((features.unsqueeze(dim=1), cap_embedding), dim=1)\n",
    "        lstm_out, self.hidden = self.lstm(embeddings)\n",
    "        outputs = self.linear(lstm_out)\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, inputs, states=None, max_len=20):\n",
    "        res = []\n",
    "        for i in range(max_len):\n",
    "            lstm_out, states = self.lstm(inputs, states)\n",
    "            outputs = self.linear(lstm_out.squeeze(dim=1))\n",
    "            _, predicted_idx = outputs.max(dim=1)\n",
    "            res.append(predicted_idx.item())\n",
    "            if predicted_idx == 1:  # Assuming 1 is the end token\n",
    "                break\n",
    "            inputs = self.embed(predicted_idx)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import device\n",
    "\n",
    "\n",
    "def train_model(encoder, decoder, data_loader, criterion, optimizer, num_epochs=5, log_file='./training_log.txt'):\n",
    "    # Open the training log file\n",
    "    f = open(log_file, \"w\")\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for i_step, (images, captions) in enumerate(data_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            decoder.zero_grad()\n",
    "            encoder.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print and log statistics\n",
    "            stats = (\n",
    "                f\"Epoch [{epoch}/{num_epochs}], Step [{i_step + 1}/{len(data_loader)}], \"\n",
    "                f\"Loss: {loss.item():.4f}, Perplexity: {np.exp(loss.item()):.4f}\"\n",
    "            )\n",
    "            f.write(stats + \"\\n\")\n",
    "            f.flush()\n",
    "            if i_step % 10 == 0:\n",
    "                print(\"\\r\" + stats, end=\"\")\n",
    "        \n",
    "        # Save the models\n",
    "        if epoch % 1 == 0:\n",
    "            torch.save(decoder.state_dict(), os.path.join(\"./models\", f\"decoder-{epoch}.pkl\"))\n",
    "            torch.save(encoder.state_dict(), os.path.join(\"./models\", f\"encoder-{epoch}.pkl\"))\n",
    "    \n",
    "    # Close the log file\n",
    "    f.close()\n",
    "\n",
    "def generate_caption(encoder, decoder, image_path, vocab):\n",
    "    image = load_image(image_path, transform)\n",
    "    features = encoder(image).unsqueeze(1)\n",
    "    output = decoder.sample(features)\n",
    "    caption = [vocab.get(idx, \"<unk>\") for idx in output]\n",
    "    return ' '.join(caption)\n",
    "\n",
    "def load_image(image_path, transform):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    return image.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters and Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = 5000  # Update with actual vocab size\n",
    "\n",
    "# File paths\n",
    "encoder_file = './models/encoder-3.pkl'\n",
    "decoder_file = './models/decoder-3.pkl'\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Load or train models\n",
    "if os.path.exists(encoder_file) and os.path.exists(decoder_file):\n",
    "    encoder.load_state_dict(torch.load(encoder_file))\n",
    "    decoder.load_state_dict(torch.load(decoder_file))\n",
    "else:\n",
    "    # Assume `data_loader` and other necessary components are defined\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()))\n",
    "    train_model(encoder, decoder, data_loader, criterion, optimizer, num_epochs=5)\n",
    "    torch.save(encoder.state_dict(), encoder_file)\n",
    "    torch.save(decoder.state_dict(), decoder_file)\n",
    "\n",
    "# Move models to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Define image transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Example vocabulary dictionary\n",
    "vocab = {0: '<start>', 1: '<end>', 2: 'a', 3: 'man', 4: 'with', 5: 'hat'}\n",
    "\n",
    "# Generate caption\n",
    "image_path = '/home/hariom/python/Image detector/cat.jpeg'\n",
    "caption = generate_caption(encoder, decoder, image_path, vocab)\n",
    "print(\"Generated Caption:\", caption)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
